{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "import numpy as np\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import accumulate\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv(r\"./all_data.tsv\", sep='\\t',escapechar='\\\\')\n",
    "# data2 = pd.read_csv(r\"./imdb_train_test.tsv\", sep='\\t')\n",
    "\n",
    "def load_stopwords(filepath):\n",
    "    stopwords_final = []\n",
    "    #2 ollect words from files. Each word is on one line\n",
    "    file = open(filepath,\"r\",encoding=\"utf-8\") #\"stopwords-master/baidu.txt\"\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        stopwords_final.append(line)\n",
    "    file.close()\n",
    "    return stopwords_final\n",
    "\n",
    "def clean_review(raw_review):\n",
    "    # 1. 评论是爬虫抓取的，存在一些 html 标签，需要去掉\n",
    "    review_text = BeautifulSoup(raw_review, 'html.parser').get_text()\n",
    "    # 2. 标点符号只保留 “-” 和 上单引号\n",
    "    review_text = rex.sub(' ', review_text)\n",
    "    # 3. 全部变成小写\n",
    "    review_text = review_text.lower()\n",
    "    # 4. 分词\n",
    "    word_list = review_text.split()\n",
    "    # 5. 词性还原\n",
    "#     tokens = list(map(lemmatizer.lemmatize, word_list))\n",
    "#     lemmatized_tokens = list(map(lambda x: lemmatizer.lemmatize(x, \"v\"), tokens))\n",
    "    # 6. 去停用词\n",
    "    meaningful_words = [x for x in word_list if x not in stop_words] #list(filter(lambda x: not x in stop_words))#, lemmatized_tokens\n",
    "    return meaningful_words\n",
    "\n",
    "stopwords_filepath = \"stop_all.txt\"\n",
    "# stop_words = set(text.ENGLISH_STOP_WORDS)\n",
    "stop_words = load_stopwords(stopwords_filepath)\n",
    "rex = re.compile(r'[!\"#$%&\\()*+,./:;<=>?@\\\\^_{|}~]+')\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sentences  = data.sentence.apply(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stuff',\n",
       " 'moment',\n",
       " 'mj',\n",
       " 'started',\n",
       " 'listening',\n",
       " 'music',\n",
       " 'watching',\n",
       " 'odd',\n",
       " 'documentary',\n",
       " 'watched',\n",
       " 'wiz',\n",
       " 'watched',\n",
       " 'moonwalker',\n",
       " 'i',\n",
       " 'insight',\n",
       " 'guy',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'cool',\n",
       " 'eighties',\n",
       " 'make',\n",
       " 'mind',\n",
       " 'guilty',\n",
       " 'innocent',\n",
       " 'moonwalker',\n",
       " 'part',\n",
       " 'biography',\n",
       " 'part',\n",
       " 'feature',\n",
       " 'film',\n",
       " 'i',\n",
       " 'remember',\n",
       " 'cinema',\n",
       " 'originally',\n",
       " 'released',\n",
       " 'subtle',\n",
       " 'messages',\n",
       " \"mj's\",\n",
       " 'feeling',\n",
       " 'press',\n",
       " 'obvious',\n",
       " 'message',\n",
       " 'drugs',\n",
       " 'bad',\n",
       " \"m'kay\",\n",
       " 'visually',\n",
       " 'impressive',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'remotely',\n",
       " 'mj',\n",
       " 'hate',\n",
       " 'find',\n",
       " 'boring',\n",
       " 'call',\n",
       " 'mj',\n",
       " 'egotist',\n",
       " 'consenting',\n",
       " 'making',\n",
       " 'movie',\n",
       " 'mj',\n",
       " 'fans',\n",
       " 'made',\n",
       " 'fans',\n",
       " 'true',\n",
       " 'nice',\n",
       " 'actual',\n",
       " 'feature',\n",
       " 'film',\n",
       " 'bit',\n",
       " 'finally',\n",
       " 'starts',\n",
       " '20',\n",
       " 'minutes',\n",
       " 'excluding',\n",
       " 'smooth',\n",
       " 'criminal',\n",
       " 'sequence',\n",
       " 'joe',\n",
       " 'pesci',\n",
       " 'convincing',\n",
       " 'psychopathic',\n",
       " 'powerful',\n",
       " 'drug',\n",
       " 'lord',\n",
       " 'mj',\n",
       " 'dead',\n",
       " 'bad',\n",
       " 'mj',\n",
       " 'overheard',\n",
       " 'plans',\n",
       " 'nah',\n",
       " 'joe',\n",
       " \"pesci's\",\n",
       " 'character',\n",
       " 'ranted',\n",
       " 'wanted',\n",
       " 'people',\n",
       " 'supplying',\n",
       " 'drugs',\n",
       " 'i',\n",
       " 'dunno',\n",
       " 'hates',\n",
       " \"mj's\",\n",
       " 'music',\n",
       " 'lots',\n",
       " 'cool',\n",
       " 'things',\n",
       " 'mj',\n",
       " 'turning',\n",
       " 'car',\n",
       " 'robot',\n",
       " 'speed',\n",
       " 'demon',\n",
       " 'sequence',\n",
       " 'director',\n",
       " 'patience',\n",
       " 'saint',\n",
       " 'filming',\n",
       " 'kiddy',\n",
       " 'bad',\n",
       " 'sequence',\n",
       " 'directors',\n",
       " 'hate',\n",
       " 'working',\n",
       " 'kid',\n",
       " 'bunch',\n",
       " 'performing',\n",
       " 'complex',\n",
       " 'dance',\n",
       " 'scene',\n",
       " 'bottom',\n",
       " 'line',\n",
       " 'movie',\n",
       " 'people',\n",
       " 'mj',\n",
       " 'level',\n",
       " 'i',\n",
       " 'people',\n",
       " 'stay',\n",
       " 'give',\n",
       " 'wholesome',\n",
       " 'message',\n",
       " 'ironically',\n",
       " \"mj's\",\n",
       " 'bestest',\n",
       " 'buddy',\n",
       " 'movie',\n",
       " 'girl',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'talented',\n",
       " 'people',\n",
       " 'grace',\n",
       " 'planet',\n",
       " 'guilty',\n",
       " 'attention',\n",
       " 'gave',\n",
       " 'subject',\n",
       " 'hmmm',\n",
       " 'i',\n",
       " 'people',\n",
       " 'closed',\n",
       " 'doors',\n",
       " 'i',\n",
       " 'fact',\n",
       " 'extremely',\n",
       " 'nice',\n",
       " 'stupid',\n",
       " 'guy',\n",
       " 'sickest',\n",
       " 'liars',\n",
       " 'i',\n",
       " 'hope']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stuff',\n",
       " 'moment',\n",
       " 'mj',\n",
       " 'started',\n",
       " 'listening',\n",
       " 'music',\n",
       " 'watching',\n",
       " 'odd',\n",
       " 'documentary',\n",
       " 'watched',\n",
       " 'wiz',\n",
       " 'watched',\n",
       " 'moonwalker',\n",
       " 'i',\n",
       " 'insight',\n",
       " 'guy',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'cool',\n",
       " 'eighties',\n",
       " 'make',\n",
       " 'mind',\n",
       " 'guilty',\n",
       " 'innocent',\n",
       " 'moonwalker',\n",
       " 'part',\n",
       " 'biography',\n",
       " 'part',\n",
       " 'feature',\n",
       " 'film',\n",
       " 'i',\n",
       " 'remember',\n",
       " 'cinema',\n",
       " 'originally',\n",
       " 'released',\n",
       " 'subtle',\n",
       " 'messages',\n",
       " \"mj's\",\n",
       " 'feeling',\n",
       " 'press',\n",
       " 'obvious',\n",
       " 'message',\n",
       " 'drugs',\n",
       " 'bad',\n",
       " \"m'kay\",\n",
       " 'visually',\n",
       " 'impressive',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'remotely',\n",
       " 'mj',\n",
       " 'hate',\n",
       " 'find',\n",
       " 'boring',\n",
       " 'call',\n",
       " 'mj',\n",
       " 'egotist',\n",
       " 'consenting',\n",
       " 'making',\n",
       " 'movie',\n",
       " 'mj',\n",
       " 'fans',\n",
       " 'made',\n",
       " 'fans',\n",
       " 'true',\n",
       " 'nice',\n",
       " 'actual',\n",
       " 'feature',\n",
       " 'film',\n",
       " 'bit',\n",
       " 'finally',\n",
       " 'starts',\n",
       " '20',\n",
       " 'minutes',\n",
       " 'excluding',\n",
       " 'smooth',\n",
       " 'criminal',\n",
       " 'sequence',\n",
       " 'joe',\n",
       " 'pesci',\n",
       " 'convincing',\n",
       " 'psychopathic',\n",
       " 'powerful',\n",
       " 'drug',\n",
       " 'lord',\n",
       " 'mj',\n",
       " 'dead',\n",
       " 'bad',\n",
       " 'mj',\n",
       " 'overheard',\n",
       " 'plans',\n",
       " 'nah',\n",
       " 'joe',\n",
       " \"pesci's\",\n",
       " 'character',\n",
       " 'ranted',\n",
       " 'wanted',\n",
       " 'people',\n",
       " 'supplying',\n",
       " 'drugs',\n",
       " 'i',\n",
       " 'dunno',\n",
       " 'hates',\n",
       " \"mj's\",\n",
       " 'music',\n",
       " 'lots',\n",
       " 'cool',\n",
       " 'things',\n",
       " 'mj',\n",
       " 'turning',\n",
       " 'car',\n",
       " 'robot',\n",
       " 'speed',\n",
       " 'demon',\n",
       " 'sequence',\n",
       " 'director',\n",
       " 'patience',\n",
       " 'saint',\n",
       " 'filming',\n",
       " 'kiddy',\n",
       " 'bad',\n",
       " 'sequence',\n",
       " 'directors',\n",
       " 'hate',\n",
       " 'working',\n",
       " 'kid',\n",
       " 'bunch',\n",
       " 'performing',\n",
       " 'complex',\n",
       " 'dance',\n",
       " 'scene',\n",
       " 'bottom',\n",
       " 'line',\n",
       " 'movie',\n",
       " 'people',\n",
       " 'mj',\n",
       " 'level',\n",
       " 'i',\n",
       " 'people',\n",
       " 'stay',\n",
       " 'give',\n",
       " 'wholesome',\n",
       " 'message',\n",
       " 'ironically',\n",
       " \"mj's\",\n",
       " 'bestest',\n",
       " 'buddy',\n",
       " 'movie',\n",
       " 'girl',\n",
       " 'michael',\n",
       " 'jackson',\n",
       " 'talented',\n",
       " 'people',\n",
       " 'grace',\n",
       " 'planet',\n",
       " 'guilty',\n",
       " 'attention',\n",
       " 'gave',\n",
       " 'subject',\n",
       " 'hmmm',\n",
       " 'i',\n",
       " 'people',\n",
       " 'closed',\n",
       " 'doors',\n",
       " 'i',\n",
       " 'fact',\n",
       " 'extremely',\n",
       " 'nice',\n",
       " 'stupid',\n",
       " 'guy',\n",
       " 'sickest',\n",
       " 'liars',\n",
       " 'i',\n",
       " 'hope',\n",
       " 'bromwell',\n",
       " 'high',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " 'ran',\n",
       " 'time',\n",
       " 'programs',\n",
       " 'school',\n",
       " 'life',\n",
       " 'teachers',\n",
       " '35',\n",
       " 'years',\n",
       " 'teaching',\n",
       " 'profession',\n",
       " 'lead',\n",
       " 'bromwell',\n",
       " \"high's\",\n",
       " 'satire',\n",
       " 'closer',\n",
       " 'reality',\n",
       " 'teachers',\n",
       " 'scramble',\n",
       " 'survive',\n",
       " 'financially',\n",
       " 'insightful',\n",
       " 'students',\n",
       " 'pathetic',\n",
       " \"teachers'\",\n",
       " 'pomp',\n",
       " 'pettiness',\n",
       " 'situation',\n",
       " 'remind',\n",
       " 'schools',\n",
       " 'i',\n",
       " 'knew',\n",
       " 'students',\n",
       " 'i',\n",
       " 'episode',\n",
       " 'student',\n",
       " 'repeatedly',\n",
       " 'burn',\n",
       " 'school',\n",
       " 'i',\n",
       " 'immediately',\n",
       " 'recalled',\n",
       " 'high',\n",
       " 'classic',\n",
       " 'line',\n",
       " 'inspector',\n",
       " 'sack',\n",
       " 'teachers',\n",
       " 'student',\n",
       " 'bromwell',\n",
       " 'high',\n",
       " 'i',\n",
       " 'expect',\n",
       " 'adults',\n",
       " 'age',\n",
       " 'bromwell',\n",
       " 'high',\n",
       " 'fetched',\n",
       " 'pity']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-552130f8ea76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcal_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcal_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_lenght'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"中位数：\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcal_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_lenght'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"均值数：\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcal_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review_lenght'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mcal_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "cal_len = pd.DataFrame()\n",
    "cal_len['review_lenght'] = list(map(len, sentences))\n",
    "print(\"中位数：\", cal_len['review_length'].median())\n",
    "print(\"均值数：\", cal_len['review_length'].mean())\n",
    "del cal_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['length'] = data['sentence'].apply(lambda x: len(x))\n",
    "len_df = data.groupby('length').count()\n",
    "sent_length = len_df.index.tolist()\n",
    "sent_freq = len_df['sentence'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 绘制句子长度及出现频数统计图\n",
    "plt.bar(sent_length, sent_freq)\n",
    "plt.title(\"The length of Sentences and the frequency of sentence length\")\n",
    "plt.xlabel(\"The length of Sentences\")\n",
    "plt.ylabel(\"The frequency of sentence length\")\n",
    "plt.savefig(\"./The length of Sentences and the frequency of sentence length.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_pentage_list = [(count/sum(sent_freq)) for count in accumulate(sent_freq)]\n",
    "plt.plot(sent_length, sent_pentage_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 寻找分位点为quantile的句子长度\n",
    "quantile = 0.91\n",
    "#print(list(sent_pentage_list))\n",
    "for length, per in zip(sent_length, sent_pentage_list):\n",
    "    if round(per, 2) == quantile:\n",
    "        index = length\n",
    "        break\n",
    "print(\"\\n分位点为%s的句子长度:%d.\" % (quantile, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制句子长度累积分布函数图\n",
    "plt.plot(sent_length, sent_pentage_list)\n",
    "plt.hlines(quantile, 0, index, colors=\"c\", linestyles=\"dashed\")\n",
    "plt.vlines(index, 0, quantile, colors=\"c\", linestyles=\"dashed\")\n",
    "plt.text(0, quantile, str(quantile))\n",
    "plt.text(index, 0, str(index))\n",
    "plt.title(\"The graph of cumulative distribution function of sentence length\")\n",
    "plt.xlabel(\"The length of sentences\")\n",
    "plt.ylabel(\"The sentence length accumulates frequency\")\n",
    "plt.savefig(\"./The graph of cumulative distribution function of sentence length.png\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('all_sentences.npy',sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding_vector_size = 100\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    size=embedding_vector_size,sg=1, hs=1,\n",
    "    min_count=2, window=5, workers=4)\n",
    "w2v_model.save(\"word2vec_100dim_sg1_hs1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_size = 200\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    size=embedding_vector_size,sg=1, hs=1,\n",
    "    min_count=2, window=5, workers=4)\n",
    "w2v_model.save(\"word2vec_\"+str(embedding_vector_size)+\"dim_sg1_hs1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 取得所有单词\n",
    "vocab_list = list(w2v_model.wv.vocab.keys())\n",
    "# 每个词语对应的索引\n",
    "word_index = {word: index for index, word in enumerate(vocab_list)}\n",
    "# 序列化\n",
    "def get_index(sentence):\n",
    "    global word_index\n",
    "    sequence = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            sequence.append(word_index[word])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return sequence\n",
    "X_data = list(map(get_index, sentences))\n",
    "\n",
    "# 截长补短\n",
    "# max_len 根据中位数和平均值得来的\n",
    "maxlen = 190\n",
    "X_pad = pad_sequences(X_data, maxlen=maxlen)\n",
    "# 取得标签\n",
    "Y = data.label.values+data2.label.values\n",
    "np.save(\"Y.npy\",Y)\n",
    "# 划分数据集\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_pad,\n",
    "    Y,\n",
    "    test_size=0.2,\n",
    "    random_state=42)\n",
    "np.save(\"X_train.npy\",X_train)\n",
    "np.save(\"X_test.npy\",X_test)\n",
    "np.save(\"Y_train.npy\",Y_train)\n",
    "np.save(\"Y_test.npy\",Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 15s 729us/step - loss: 0.7161 - acc: 0.5023 - val_loss: 0.6897 - val_acc: 0.5322\n",
      "Epoch 2/20\n",
      " 5120/20000 [======>.......................] - ETA: 8s - loss: 0.7102 - acc: 0.5066"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-dc265d339955>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m ) \n\u001b[1;32m     33\u001b[0m \u001b[0;31m# model_save_path= './labelled_train_test_model.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense\n",
    "\n",
    "# 让 Keras 的 Embedding 层使用训练好的Word2Vec权重\n",
    "embedding_matrix = w2v_model.wv.vectors\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(\n",
    "    input_dim=embedding_matrix.shape[0],\n",
    "    output_dim=embedding_matrix.shape[1],\n",
    "    input_length=maxlen,\n",
    "    weights=[embedding_matrix],\n",
    "    trainable=False))\n",
    "model.add(Bidirectional(LSTM(128, recurrent_dropout=0.1)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=Y_train,\n",
    "    validation_data=(X_test, Y_test),\n",
    "    batch_size=1024,\n",
    "    epochs=20\n",
    ") \n",
    "# model_save_path= './labelled_train_test_model.h5'\n",
    "model_save_path= './imdb_train_test_model_BILSTM1.h5'\n",
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(his):\n",
    "    loss = his.history['loss']\n",
    "    val_loss = his.history['val_loss']\n",
    "    acc = his.history['acc']\n",
    "    val_acc = his.history['val_acc']\n",
    "\n",
    "    # make a figure\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    # subplot loss\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax1.plot(loss,label='train_loss')\n",
    "    ax1.plot(val_loss,label='val_loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Loss on Training and Validation Data')\n",
    "    ax1.legend()\n",
    "    # subplot acc\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    ax2.plot(acc,label='train_acc')\n",
    "    ax2.plot(val_acc,label='val_acc')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_title('Accuracy  on Training and Validation Data')\n",
    "    ax2.legend()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,title=\"Confusion Matrix\",cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    The function prints and plots the confusion matrix\n",
    "    \"\"\"\n",
    "    plt.imshow(cm,interpolation=\"nearest\",cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks,classes,rotation=0)\n",
    "    plt.yticks(tick_marks,classes)\n",
    "    \n",
    "    thresh = cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i,j],horizontalalignment = 'center',\n",
    "        color=\"white\" if cm[i,j]>thresh else\"black\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = model.predict_classes(X_test)\n",
    "cnf_matrix = confusion_matrix(Y_test, y_pred)\n",
    "accuracy = (cnf_matrix[0,0]+cnf_matrix[1,1])/(cnf_matrix[0,0]+cnf_matrix[0,1]+cnf_matrix[1,0]+cnf_matrix[1,1])\n",
    "precision = cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[0,1])\n",
    "recall = cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[1,0])\n",
    "specificity = cnf_matrix[0,0]/(cnf_matrix[0,0]+cnf_matrix[0,1])\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(\"accuracy metric in the testing dataset: \", accuracy)\n",
    "print(\"precision metric in the testing dataset: \", precision)\n",
    "print(\"recall metric in the testing dataset: \", recall)\n",
    "print(\"specificity metric in the testing dataset: \", specificity)\n",
    "print(\"F1 score metric in the testing dataset: \", f1)\n",
    "class_names=[0,1]\n",
    "plt.figure\n",
    "plot_confusion_matrix(cnf_matrix,classes=class_names, title=\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 序列化\n",
    "def get_predict_index(sentence,word_index):\n",
    "    sequence = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            sequence.append(word_index[word])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text,word2_vecpath,model):\n",
    "    stopwords = load_stopwords(\"stop_all.txt\")\n",
    "    sentence = clean_review(text)\n",
    "    w2v = Word2Vec.load(word2_vecpath)\n",
    "    # 取得所有单词\n",
    "    vocab_list = list(w2v_model.wv.vocab.keys())\n",
    "    # 每个词语对应的索引\n",
    "    word_index = {word: index for index, word in enumerate(vocab_list)}\n",
    "    seq_sentence = get_predict_index(sentence,word_index)\n",
    "    maxlen = 90\n",
    "    X_pad = pad_sequences(sentence, maxlen=maxlen)\n",
    "    y_pred = model.predict_classes(X_pad)\n",
    "    t_prob = model.predict_proba(X_pad)\n",
    "    print(y_pred,t_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
